{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96391385",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96391385",
        "outputId": "aba33776-fb80-4d85-ce11-813a34dcc283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b64ff1f5",
      "metadata": {
        "id": "b64ff1f5"
      },
      "outputs": [],
      "source": [
        "project_path='/content/gdrive/MyDrive/Colab Notebooks'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "404590ac",
      "metadata": {
        "id": "404590ac"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "# Add the project path to sys.path to allow importing from utils.py\n",
        "# project_path is defined in an earlier cell as '/content/gdrive/MyDrive/Colab Notebooks'\n",
        "if project_path not in sys.path:\n",
        "    sys.path.append(project_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "376f6040",
      "metadata": {
        "id": "376f6040"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b7c6118",
      "metadata": {
        "id": "3b7c6118"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca502a30",
      "metadata": {
        "id": "ca502a30"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d9575d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d9575d7",
        "outputId": "c0cf484a-4a8d-4fa1-e0ed-8cff6de4b2bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of character: 12633030\n",
            "Total number of character: 12633030\n",
            "<|start|>\n",
            "\n",
            "\n",
            "<|title|>The Cryptic Code</|title|>\n",
            "\n",
            "In the heart of Silicon Valley, a tech genius named\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "file_name = \"data/training_data.txt\"\n",
        "dataset_file_path = os.path.join(project_path, file_name)\n",
        "\n",
        "with open(dataset_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text_data = f.read()\n",
        "\n",
        "print(\"Total number of character:\", len(text_data))\n",
        "\n",
        "\n",
        "# data_limit = len(text_data) // 1\n",
        "\n",
        "#text_data=text_data[:data_limit]\n",
        "print(\"Total number of character:\", len(text_data))\n",
        "print(text_data[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VxjKKoQuCht-",
      "metadata": {
        "id": "VxjKKoQuCht-"
      },
      "source": [
        "### **Define GPT Configuration**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-CQuBXj9CWyA",
      "metadata": {
        "id": "-CQuBXj9CWyA"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fxfw-sooCXVE",
      "metadata": {
        "id": "fxfw-sooCXVE"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 512, # Context length (Original contex length in GPT 2  is 1024. For the sake of simplicity we used 256)\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False,       # Query-Key-Value bias\n",
        "    \"key_dim\": 768,\n",
        "    \"val_dim\": 768\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b31bdb3",
      "metadata": {
        "id": "3b31bdb3"
      },
      "source": [
        "### Tokenize using `tiktoken`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c71f168",
      "metadata": {
        "id": "0c71f168"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d240d38e",
      "metadata": {
        "id": "d240d38e"
      },
      "source": [
        "### Encode tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39a16781",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39a16781",
        "outputId": "675ac728-82a0-4af3-9581-005b934a67c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Characters: 12633030\n",
            "Tokens: 3049258\n"
          ]
        }
      ],
      "source": [
        "total_characters = len(text_data)\n",
        "total_tokens = len(tokenizer.encode(text_data))\n",
        "\n",
        "print(\"Characters:\", total_characters)\n",
        "print(\"Tokens:\", total_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f590181",
      "metadata": {
        "id": "1f590181"
      },
      "outputs": [],
      "source": [
        "def text_to_token_ids(text, tokenizer):\n",
        "    # Adding your book tokens to allowed_special\n",
        "    # Updated to use only start and end tokens\n",
        "    encoded = tokenizer.encode(text, allowed_special={\n",
        "        '<|start|>',\n",
        "        '<|end|>',\n",
        "        '<|title|>',\n",
        "        '</|title|>'\n",
        "    })\n",
        "\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    if token_ids is None:\n",
        "        raise ValueError(\"token_ids is None! Check your text_to_token_ids function.\")\n",
        "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c423878d",
      "metadata": {
        "id": "c423878d"
      },
      "source": [
        "### Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2999b3f6",
      "metadata": {
        "id": "2999b3f6"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class GPTDataset(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text with updated special tokens\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\n",
        "            '<|start|>',\n",
        "            '<|end|>',\n",
        "            '<|title|>',\n",
        "            '</|title|>'\n",
        "        })\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "\n",
        "def create_dataloader(txt, batch_size=100, max_length=512,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDataset(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader( # torch.utils.data.DataLoader loads data from a Dataset in mini-batches, optionally shuffling, using multiple workers.\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38ad35a5",
      "metadata": {
        "id": "38ad35a5"
      },
      "source": [
        "### Train-Val Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d4cd23b",
      "metadata": {
        "id": "7d4cd23b"
      },
      "outputs": [],
      "source": [
        "# Train/validation ratio\n",
        "train_ratio = 0.9\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]\n",
        "\n",
        "batch_size=8\n",
        "\n",
        "torch.manual_seed(9999)\n",
        "\n",
        "train_loader = create_dataloader( # returns batch_data, batch_labels\n",
        "    train_data,\n",
        "    batch_size=batch_size,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader( # returns batch_data, batch_labels\n",
        "    val_data,\n",
        "    batch_size=batch_size,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1602128",
      "metadata": {
        "id": "f1602128"
      },
      "outputs": [],
      "source": [
        "# Sanity check\n",
        "\n",
        "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
        "    print(\"Not enough tokens for the training loader. \"\n",
        "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
        "          \"increase the `training_ratio`\")\n",
        "\n",
        "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
        "    print(\"Not enough tokens for the validation loader. \"\n",
        "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
        "          \"decrease the `training_ratio`\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bde57d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bde57d9",
        "outputId": "beb265c1-edb8-494a-c684-2ff0177716fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of batches in train and val loader:  661 84\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "print(\"Train loader:\")\n",
        "for x, y in train_loader: # for batch_data, batch_labels in dataloader: pass\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\nValidation loader:\")\n",
        "for x, y in val_loader:   # for batch_data, batch_labels in dataloader: pass\n",
        "    print(x.shape, y.shape)\n",
        "'''\n",
        "print('Number of batches in train and val loader: ',len(train_loader),len(val_loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11c89454",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11c89454",
        "outputId": "609402d1-7d86-45d8-bd55-98c94b2b80ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training tokens: 2707456\n",
            "Validation tokens: 341504\n",
            "All tokens: 3048960\n"
          ]
        }
      ],
      "source": [
        "train_tokens = 0\n",
        "for input_batch, target_batch in train_loader:\n",
        "    train_tokens += input_batch.numel() # returns how many total scalar values are inside a tensor.\n",
        "\n",
        "val_tokens = 0\n",
        "for input_batch, target_batch in val_loader:\n",
        "    val_tokens += input_batch.numel() # returns how many total scalar values are inside a tensor.\n",
        "\n",
        "print(\"Training tokens:\", train_tokens)\n",
        "print(\"Validation tokens:\", val_tokens)\n",
        "print(\"All tokens:\", train_tokens + val_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b84dd49",
      "metadata": {
        "id": "8b84dd49"
      },
      "outputs": [],
      "source": [
        "def calculate_batch_loss(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calculate_average_loss_per_batch(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\") # NaN is not equal to anything\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calculate_batch_loss(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13c115c2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13c115c2",
        "outputId": "a264bff8-7cb9-4420-d41f-7ea58c7d2f6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(512, 768)\n",
              "  (drop_emb): Dropout()\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout()\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout()\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout()\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout()\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout()\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout()\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout()\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout()\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout()\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout()\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout()\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout()\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout()\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout()\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout()\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout()\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout()\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout()\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout()\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout()\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout()\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout()\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout()\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout()\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from utils import GPTModel\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a34f9f4a",
      "metadata": {
        "id": "a34f9f4a"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "    # For-loop: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56a50d0a",
      "metadata": {
        "id": "56a50d0a"
      },
      "source": [
        ">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56f619ee",
      "metadata": {
        "id": "56f619ee"
      },
      "source": [
        "## **Save checkpoints from Colab before timeout**\n",
        "1. Periodic saving: Saves every save_freq steps (default 50)  \n",
        "2. Epoch saving: Saves after each complete epoch  \n",
        "3. Complete state: Saves model, optimizer, losses, and training progress  \n",
        "4. Resume capability: Can continue training from any checkpoint  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50f00b27",
      "metadata": {
        "id": "50f00b27"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# This removes everything inside the folder but not the folder itself\n",
        "def clean_folder(folder_path):\n",
        "  for item in os.listdir(folder_path):\n",
        "      item_path = os.path.join(folder_path, item)\n",
        "      if os.path.isfile(item_path) or os.path.islink(item_path):\n",
        "          os.unlink(item_path)\n",
        "      else:\n",
        "          shutil.rmtree(item_path)\n",
        "\n",
        "\n",
        "# Save model checkpoint with all training state\n",
        "def save_checkpoint(model, optimizer, epoch, global_step,\n",
        "                   train_losses, val_losses, track_tokens_seen,\n",
        "                   tokens_seen, checkpoint_dir,checkpoint_file):\n",
        "\n",
        "\n",
        "    new_checkpoint_path = os.path.join(checkpoint_dir, checkpoint_file)\n",
        "\n",
        "\n",
        "\n",
        "    # Save checkpoint\n",
        "\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'global_step': global_step,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'track_tokens_seen': track_tokens_seen,\n",
        "        'tokens_seen': tokens_seen,\n",
        "    }\n",
        "    torch.save(checkpoint, new_checkpoint_path)\n",
        "\n",
        "\n",
        "# Load checkpoint and resume training\n",
        "def load_checkpoint(model, optimizer, checkpoint_path):\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    epoch = checkpoint['epoch']\n",
        "    global_step = checkpoint['global_step']\n",
        "    train_losses = checkpoint['train_losses']\n",
        "    val_losses = checkpoint['val_losses']\n",
        "    track_tokens_seen = checkpoint['track_tokens_seen']\n",
        "    tokens_seen = checkpoint['tokens_seen']\n",
        "\n",
        "    print(f\"Loaded checkpoint from epoch {epoch+1}, step {global_step}\")\n",
        "    print(f\"Tokens seen so far: {tokens_seen:,}\")\n",
        "\n",
        "    return epoch, global_step, train_losses, val_losses, track_tokens_seen, tokens_seen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39603e55",
      "metadata": {
        "id": "39603e55"
      },
      "source": [
        "## **Training Loop For LLM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dce7fbd4",
      "metadata": {
        "id": "dce7fbd4"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calculate_average_loss_per_batch(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calculate_average_loss_per_batch(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0b4c85f",
      "metadata": {
        "id": "c0b4c85f"
      },
      "outputs": [],
      "source": [
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7877170",
      "metadata": {
        "id": "c7877170"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer,\n",
        "                       checkpoint_dir=\"checkpoints\", save_freq=None,resume_from=None):\n",
        "\n",
        "\n",
        "    # Create checkpoint directory if it doesn't exist\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize or load training state\n",
        "\n",
        "    if resume_from :\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, resume_from)\n",
        "\n",
        "        if os.path.exists(checkpoint_path):\n",
        "          print(f\"Resuming training from: {resume_from}\")\n",
        "          start_epoch, global_step, train_losses, val_losses, track_tokens_seen, tokens_seen = \\\n",
        "              load_checkpoint(model, optimizer,checkpoint_path)\n",
        "          start_epoch += 1  # Start from next epoch\n",
        "    else:\n",
        "        print(\"Starting fresh training\")\n",
        "\n",
        "        # Initialize lists to track losses and tokens seen\n",
        "        start_epoch = 0\n",
        "        global_step = -1\n",
        "        train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "        tokens_seen = 0\n",
        "\n",
        "    # Training loop\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = calculate_batch_loss(input_batch, target_batch, model, device)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # Evaluation step (Optional)\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "            # Save checkpoint periodically\n",
        "            if save_freq is not None and global_step > 0 and global_step % save_freq == 0:\n",
        "                checkpoint_file=f\"checkpoint_epoch_{epoch+1}_step_{global_step}.pt\"\n",
        "\n",
        "                # Delete all previous checkpoints\n",
        "                clean_folder(checkpoint_dir)\n",
        "\n",
        "                # save checkpoint\n",
        "                save_checkpoint(model, optimizer, epoch, global_step,\n",
        "                              train_losses, val_losses, track_tokens_seen,\n",
        "                              tokens_seen, checkpoint_dir,checkpoint_file)\n",
        "                print(f\"Checkpoint saved at step {global_step}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        print(f\"\\nSample generation after epoch {epoch+1}:\")\n",
        "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
        "\n",
        "        # Save checkpoint after each epoch\n",
        "        checkpoint_file = f\"checkpoint_V1_2_epoch_{epoch+1}_step_{global_step}.pt\"\n",
        "        # Delete all previous checkpoints\n",
        "        clean_folder(checkpoint_dir)\n",
        "\n",
        "        if epoch & 1:\n",
        "          save_checkpoint(model, optimizer, epoch, global_step,\n",
        "                              train_losses, val_losses, track_tokens_seen,\n",
        "                              tokens_seen, checkpoint_dir,checkpoint_file)\n",
        "\n",
        "          print(f\"Epoch {epoch+1} checkpoint saved\")\n",
        "\n",
        "        elapsed_time = (time.time() - start_time) / 60\n",
        "        print(f\"Epoch {epoch+1} completed in {elapsed_time:.2f} minutes\")\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SxxLlbJHC0mw",
      "metadata": {
        "id": "SxxLlbJHC0mw"
      },
      "source": [
        "START FRESH TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FmIx5xsoCyKD",
      "metadata": {
        "id": "FmIx5xsoCyKD"
      },
      "outputs": [],
      "source": [
        "def start_fresh_training(num_epochs=12):\n",
        "\n",
        "    torch.manual_seed(123)\n",
        "\n",
        "    model = GPTModel(GPT_CONFIG_124M)\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "    train_losses, val_losses, tokens_seen = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        optimizer=optimizer,\n",
        "        device=device,\n",
        "        num_epochs=num_epochs,\n",
        "        eval_freq=10,\n",
        "        eval_iter=10,\n",
        "        start_context=\"<|start|>\",\n",
        "        tokenizer=tokenizer,\n",
        "        checkpoint_dir=project_path+'/gpt_checkpoints',\n",
        "        # save_freq=50,\n",
        "        resume_from=None  # Start fresh\n",
        "    )\n",
        "\n",
        "    return model, train_losses, val_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9esEE2pQC6fy",
      "metadata": {
        "id": "9esEE2pQC6fy"
      },
      "source": [
        "RESUME FROM CHECKPOINT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M9aONOoNC49H",
      "metadata": {
        "id": "M9aONOoNC49H"
      },
      "outputs": [],
      "source": [
        "\n",
        "def resume_training_from_checkpoint(checkpoint_dir,checkpoint_file,num_epochs=12):\n",
        "\n",
        "    torch.manual_seed(123)\n",
        "\n",
        "    model = GPTModel(GPT_CONFIG_124M)\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "    # Resume from the checkpoint\n",
        "\n",
        "\n",
        "    train_losses, val_losses, tokens_seen = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        optimizer=optimizer,\n",
        "        device=device,\n",
        "        num_epochs=num_epochs,\n",
        "        eval_freq=10,\n",
        "        eval_iter=10,\n",
        "        start_context=\"<|start|>\",\n",
        "        tokenizer=tokenizer,\n",
        "        checkpoint_dir=checkpoint_dir,\n",
        "\n",
        "        # save_freq=50,\n",
        "        resume_from=checkpoint_file  # Resume from here, this file/saved model\n",
        "    )\n",
        "\n",
        "    return model, train_losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c86e0371",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c86e0371",
        "outputId": "25778ff7-2249-4d46-a272-ac232f71da3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resuming training from: checkpoint_V1_2_epoch_2_step_1321.pt\n",
            "Loaded checkpoint from epoch 2, step 1321\n",
            "Tokens seen so far: 5,414,912\n",
            "\n",
            "==================================================\n",
            "Epoch 3/12\n",
            "==================================================\n",
            "Ep 3 (Step 001330): Train loss 3.777, Val loss 5.223\n",
            "Ep 3 (Step 001340): Train loss 3.759, Val loss 5.256\n",
            "Ep 3 (Step 001350): Train loss 3.828, Val loss 5.211\n",
            "Ep 3 (Step 001360): Train loss 3.835, Val loss 5.263\n",
            "Ep 3 (Step 001370): Train loss 3.718, Val loss 5.264\n",
            "Ep 3 (Step 001380): Train loss 3.710, Val loss 5.267\n",
            "Ep 3 (Step 001390): Train loss 3.675, Val loss 5.288\n",
            "Ep 3 (Step 001400): Train loss 3.656, Val loss 5.224\n",
            "Ep 3 (Step 001410): Train loss 3.682, Val loss 5.261\n",
            "Ep 3 (Step 001420): Train loss 3.741, Val loss 5.220\n",
            "Ep 3 (Step 001430): Train loss 3.687, Val loss 5.174\n",
            "Ep 3 (Step 001440): Train loss 3.753, Val loss 5.254\n",
            "Ep 3 (Step 001450): Train loss 3.739, Val loss 5.209\n",
            "Ep 3 (Step 001460): Train loss 3.761, Val loss 5.237\n",
            "Ep 3 (Step 001470): Train loss 3.630, Val loss 5.264\n",
            "Ep 3 (Step 001480): Train loss 3.669, Val loss 5.221\n",
            "Ep 3 (Step 001490): Train loss 3.654, Val loss 5.161\n",
            "Ep 3 (Step 001500): Train loss 3.647, Val loss 5.142\n",
            "Ep 3 (Step 001510): Train loss 3.672, Val loss 5.159\n",
            "Ep 3 (Step 001520): Train loss 3.628, Val loss 5.137\n",
            "Ep 3 (Step 001530): Train loss 3.632, Val loss 5.177\n",
            "Ep 3 (Step 001540): Train loss 3.618, Val loss 5.128\n",
            "Ep 3 (Step 001550): Train loss 3.573, Val loss 5.265\n",
            "Ep 3 (Step 001560): Train loss 3.623, Val loss 5.146\n",
            "Ep 3 (Step 001570): Train loss 3.702, Val loss 5.152\n",
            "Ep 3 (Step 001580): Train loss 3.467, Val loss 5.181\n",
            "Ep 3 (Step 001590): Train loss 3.538, Val loss 5.149\n",
            "Ep 3 (Step 001600): Train loss 3.577, Val loss 5.122\n",
            "Ep 3 (Step 001610): Train loss 3.542, Val loss 5.148\n",
            "Ep 3 (Step 001620): Train loss 3.673, Val loss 5.163\n",
            "Ep 3 (Step 001630): Train loss 3.554, Val loss 5.159\n",
            "Ep 3 (Step 001640): Train loss 3.506, Val loss 5.169\n",
            "Ep 3 (Step 001650): Train loss 3.656, Val loss 5.112\n",
            "Ep 3 (Step 001660): Train loss 3.541, Val loss 5.120\n",
            "Ep 3 (Step 001670): Train loss 3.494, Val loss 5.110\n",
            "Ep 3 (Step 001680): Train loss 3.548, Val loss 5.113\n",
            "Ep 3 (Step 001690): Train loss 3.572, Val loss 5.133\n",
            "Ep 3 (Step 001700): Train loss 3.483, Val loss 5.145\n",
            "Ep 3 (Step 001710): Train loss 3.446, Val loss 5.109\n",
            "Ep 3 (Step 001720): Train loss 3.574, Val loss 5.137\n",
            "Ep 3 (Step 001730): Train loss 3.449, Val loss 5.135\n",
            "Ep 3 (Step 001740): Train loss 3.476, Val loss 5.113\n",
            "Ep 3 (Step 001750): Train loss 3.409, Val loss 5.149\n",
            "Ep 3 (Step 001760): Train loss 3.384, Val loss 5.247\n",
            "Ep 3 (Step 001770): Train loss 3.329, Val loss 5.100\n",
            "Ep 3 (Step 001780): Train loss 3.414, Val loss 5.092\n",
            "Ep 3 (Step 001790): Train loss 3.478, Val loss 5.064\n",
            "Ep 3 (Step 001800): Train loss 3.327, Val loss 5.045\n",
            "Ep 3 (Step 001810): Train loss 3.390, Val loss 5.080\n",
            "Ep 3 (Step 001820): Train loss 3.237, Val loss 5.091\n",
            "Ep 3 (Step 001830): Train loss 3.465, Val loss 5.093\n",
            "Ep 3 (Step 001840): Train loss 3.404, Val loss 5.092\n",
            "Ep 3 (Step 001850): Train loss 3.434, Val loss 5.050\n",
            "Ep 3 (Step 001860): Train loss 3.526, Val loss 5.132\n",
            "Ep 3 (Step 001870): Train loss 3.355, Val loss 5.103\n",
            "Ep 3 (Step 001880): Train loss 3.281, Val loss 5.092\n",
            "Ep 3 (Step 001890): Train loss 3.386, Val loss 5.063\n",
            "Ep 3 (Step 001900): Train loss 3.412, Val loss 5.114\n",
            "Ep 3 (Step 001910): Train loss 3.240, Val loss 5.074\n",
            "Ep 3 (Step 001920): Train loss 3.345, Val loss 5.053\n",
            "Ep 3 (Step 001930): Train loss 3.305, Val loss 5.055\n",
            "Ep 3 (Step 001940): Train loss 3.361, Val loss 5.067\n",
            "Ep 3 (Step 001950): Train loss 3.384, Val loss 5.041\n",
            "Ep 3 (Step 001960): Train loss 3.285, Val loss 5.029\n",
            "Ep 3 (Step 001970): Train loss 3.332, Val loss 5.044\n",
            "Ep 3 (Step 001980): Train loss 3.159, Val loss 5.042\n",
            "\n",
            "Sample generation after epoch 3:\n",
            "<|start|>   <|title|>The Chronicles of the Lost Kingdom</|title|>  In the year 2075, a young woman named Lila was a young woman named Lila. She was a skilled herbalist who had a unique\n",
            "Epoch 3 completed in 23.51 minutes\n",
            "\n",
            "==================================================\n",
            "Epoch 4/12\n",
            "==================================================\n",
            "Ep 4 (Step 001990): Train loss 3.166, Val loss 5.043\n",
            "Ep 4 (Step 002000): Train loss 3.264, Val loss 5.043\n",
            "Ep 4 (Step 002010): Train loss 3.222, Val loss 5.060\n",
            "Ep 4 (Step 002020): Train loss 3.124, Val loss 5.060\n",
            "Ep 4 (Step 002030): Train loss 3.191, Val loss 5.071\n",
            "Ep 4 (Step 002040): Train loss 3.149, Val loss 5.041\n",
            "Ep 4 (Step 002050): Train loss 3.238, Val loss 5.060\n",
            "Ep 4 (Step 002060): Train loss 3.146, Val loss 5.049\n",
            "Ep 4 (Step 002070): Train loss 3.176, Val loss 5.086\n",
            "Ep 4 (Step 002080): Train loss 3.082, Val loss 5.079\n",
            "Ep 4 (Step 002090): Train loss 3.090, Val loss 5.090\n",
            "Ep 4 (Step 002100): Train loss 3.139, Val loss 5.066\n",
            "Ep 4 (Step 002110): Train loss 3.089, Val loss 5.071\n",
            "Ep 4 (Step 002120): Train loss 3.219, Val loss 5.037\n",
            "Ep 4 (Step 002130): Train loss 3.189, Val loss 5.059\n",
            "Ep 4 (Step 002140): Train loss 3.099, Val loss 5.018\n",
            "Ep 4 (Step 002150): Train loss 3.195, Val loss 5.038\n",
            "Ep 4 (Step 002160): Train loss 3.253, Val loss 5.047\n",
            "Ep 4 (Step 002170): Train loss 3.066, Val loss 5.058\n",
            "Ep 4 (Step 002180): Train loss 2.982, Val loss 5.052\n",
            "Ep 4 (Step 002190): Train loss 3.050, Val loss 5.052\n",
            "Ep 4 (Step 002200): Train loss 2.923, Val loss 5.054\n",
            "Ep 4 (Step 002210): Train loss 3.162, Val loss 5.081\n",
            "Ep 4 (Step 002220): Train loss 2.947, Val loss 5.051\n",
            "Ep 4 (Step 002230): Train loss 3.026, Val loss 5.085\n",
            "Ep 4 (Step 002240): Train loss 3.037, Val loss 5.064\n",
            "Ep 4 (Step 002250): Train loss 3.102, Val loss 5.061\n",
            "Ep 4 (Step 002260): Train loss 3.150, Val loss 5.054\n",
            "Ep 4 (Step 002270): Train loss 3.036, Val loss 5.048\n",
            "Ep 4 (Step 002280): Train loss 3.044, Val loss 5.033\n",
            "Ep 4 (Step 002290): Train loss 3.088, Val loss 5.016\n",
            "Ep 4 (Step 002300): Train loss 3.060, Val loss 5.038\n",
            "Ep 4 (Step 002310): Train loss 3.025, Val loss 5.033\n",
            "Ep 4 (Step 002320): Train loss 2.959, Val loss 5.012\n",
            "Ep 4 (Step 002330): Train loss 2.971, Val loss 5.045\n",
            "Ep 4 (Step 002340): Train loss 3.048, Val loss 5.028\n",
            "Ep 4 (Step 002350): Train loss 2.933, Val loss 5.029\n",
            "Ep 4 (Step 002360): Train loss 3.065, Val loss 5.018\n",
            "Ep 4 (Step 002370): Train loss 2.863, Val loss 5.027\n",
            "Ep 4 (Step 002380): Train loss 3.047, Val loss 5.011\n",
            "Ep 4 (Step 002390): Train loss 2.957, Val loss 5.027\n",
            "Ep 4 (Step 002400): Train loss 3.041, Val loss 4.997\n",
            "Ep 4 (Step 002410): Train loss 2.851, Val loss 5.029\n",
            "Ep 4 (Step 002420): Train loss 2.944, Val loss 5.031\n",
            "Ep 4 (Step 002430): Train loss 2.963, Val loss 5.025\n",
            "Ep 4 (Step 002440): Train loss 2.908, Val loss 5.015\n",
            "Ep 4 (Step 002450): Train loss 2.827, Val loss 5.053\n",
            "Ep 4 (Step 002460): Train loss 2.918, Val loss 5.083\n",
            "Ep 4 (Step 002470): Train loss 2.978, Val loss 5.035\n",
            "Ep 4 (Step 002480): Train loss 2.805, Val loss 5.046\n",
            "Ep 4 (Step 002490): Train loss 2.985, Val loss 5.060\n",
            "Ep 4 (Step 002500): Train loss 2.898, Val loss 5.003\n",
            "Ep 4 (Step 002510): Train loss 2.800, Val loss 5.007\n",
            "Ep 4 (Step 002520): Train loss 2.884, Val loss 5.009\n",
            "Ep 4 (Step 002530): Train loss 2.726, Val loss 5.016\n",
            "Ep 4 (Step 002540): Train loss 2.853, Val loss 5.053\n",
            "Ep 4 (Step 002550): Train loss 2.879, Val loss 5.011\n",
            "Ep 4 (Step 002560): Train loss 2.928, Val loss 5.039\n",
            "Ep 4 (Step 002570): Train loss 2.912, Val loss 4.996\n",
            "Ep 4 (Step 002580): Train loss 2.832, Val loss 4.997\n",
            "Ep 4 (Step 002590): Train loss 2.804, Val loss 5.004\n",
            "Ep 4 (Step 002600): Train loss 2.774, Val loss 5.008\n",
            "Ep 4 (Step 002610): Train loss 2.723, Val loss 5.012\n",
            "Ep 4 (Step 002620): Train loss 2.888, Val loss 5.020\n",
            "Ep 4 (Step 002630): Train loss 2.841, Val loss 4.988\n",
            "Ep 4 (Step 002640): Train loss 2.820, Val loss 4.985\n",
            "\n",
            "Sample generation after epoch 4:\n",
            "<|start|>   <|title|>The Chronicles of the Lost Relic</|title|>  In the year 2250, the world was once again on the brink of extinction. The world had become a place where humans lived in harmony, and\n",
            "Epoch 4 checkpoint saved\n",
            "Epoch 4 completed in 47.53 minutes\n",
            "\n",
            "==================================================\n",
            "Epoch 5/12\n",
            "==================================================\n",
            "Ep 5 (Step 002650): Train loss 2.703, Val loss 5.011\n",
            "Ep 5 (Step 002660): Train loss 2.792, Val loss 5.005\n",
            "Ep 5 (Step 002670): Train loss 2.715, Val loss 5.019\n",
            "Ep 5 (Step 002680): Train loss 2.827, Val loss 5.029\n",
            "Ep 5 (Step 002690): Train loss 2.785, Val loss 5.038\n",
            "Ep 5 (Step 002700): Train loss 2.693, Val loss 5.024\n",
            "Ep 5 (Step 002710): Train loss 2.670, Val loss 5.050\n",
            "Ep 5 (Step 002720): Train loss 2.777, Val loss 5.043\n",
            "Ep 5 (Step 002730): Train loss 2.692, Val loss 5.067\n",
            "Ep 5 (Step 002740): Train loss 2.658, Val loss 5.100\n",
            "Ep 5 (Step 002750): Train loss 2.665, Val loss 5.068\n",
            "Ep 5 (Step 002760): Train loss 2.681, Val loss 5.081\n",
            "Ep 5 (Step 002770): Train loss 2.645, Val loss 5.057\n",
            "Ep 5 (Step 002780): Train loss 2.608, Val loss 5.058\n",
            "Ep 5 (Step 002790): Train loss 2.732, Val loss 5.068\n",
            "Ep 5 (Step 002800): Train loss 2.747, Val loss 5.068\n",
            "Ep 5 (Step 002810): Train loss 2.725, Val loss 5.060\n",
            "Ep 5 (Step 002820): Train loss 2.561, Val loss 5.070\n",
            "Ep 5 (Step 002830): Train loss 2.685, Val loss 5.052\n",
            "Ep 5 (Step 002840): Train loss 2.603, Val loss 5.061\n",
            "Ep 5 (Step 002850): Train loss 2.563, Val loss 5.056\n",
            "Ep 5 (Step 002860): Train loss 2.650, Val loss 5.067\n",
            "Ep 5 (Step 002870): Train loss 2.656, Val loss 5.065\n",
            "Ep 5 (Step 002880): Train loss 2.682, Val loss 5.056\n",
            "Ep 5 (Step 002890): Train loss 2.540, Val loss 5.081\n",
            "Ep 5 (Step 002900): Train loss 2.567, Val loss 5.081\n",
            "Ep 5 (Step 002910): Train loss 2.619, Val loss 5.066\n",
            "Ep 5 (Step 002920): Train loss 2.559, Val loss 5.085\n",
            "Ep 5 (Step 002930): Train loss 2.553, Val loss 5.078\n",
            "Ep 5 (Step 002940): Train loss 2.564, Val loss 5.080\n",
            "Ep 5 (Step 002950): Train loss 2.512, Val loss 5.109\n",
            "Ep 5 (Step 002960): Train loss 2.541, Val loss 5.073\n",
            "Ep 5 (Step 002970): Train loss 2.554, Val loss 5.066\n",
            "Ep 5 (Step 002980): Train loss 2.404, Val loss 5.062\n",
            "Ep 5 (Step 002990): Train loss 2.505, Val loss 5.074\n",
            "Ep 5 (Step 003000): Train loss 2.517, Val loss 5.078\n",
            "Ep 5 (Step 003010): Train loss 2.658, Val loss 5.073\n",
            "Ep 5 (Step 003020): Train loss 2.433, Val loss 5.055\n",
            "Ep 5 (Step 003030): Train loss 2.469, Val loss 5.077\n",
            "Ep 5 (Step 003040): Train loss 2.528, Val loss 5.072\n",
            "Ep 5 (Step 003050): Train loss 2.443, Val loss 5.067\n",
            "Ep 5 (Step 003060): Train loss 2.556, Val loss 5.097\n",
            "Ep 5 (Step 003070): Train loss 2.494, Val loss 5.066\n",
            "Ep 5 (Step 003080): Train loss 2.429, Val loss 5.056\n",
            "Ep 5 (Step 003090): Train loss 2.504, Val loss 5.051\n",
            "Ep 5 (Step 003100): Train loss 2.499, Val loss 5.051\n",
            "Ep 5 (Step 003110): Train loss 2.358, Val loss 5.067\n",
            "Ep 5 (Step 003120): Train loss 2.533, Val loss 5.039\n",
            "Ep 5 (Step 003130): Train loss 2.438, Val loss 5.077\n",
            "Ep 5 (Step 003140): Train loss 2.427, Val loss 5.065\n",
            "Ep 5 (Step 003150): Train loss 2.519, Val loss 5.075\n",
            "Ep 5 (Step 003160): Train loss 2.267, Val loss 5.068\n",
            "Ep 5 (Step 003170): Train loss 2.373, Val loss 5.061\n",
            "Ep 5 (Step 003180): Train loss 2.434, Val loss 5.076\n",
            "Ep 5 (Step 003190): Train loss 2.441, Val loss 5.049\n",
            "Ep 5 (Step 003200): Train loss 2.463, Val loss 5.052\n",
            "Ep 5 (Step 003210): Train loss 2.317, Val loss 5.061\n",
            "Ep 5 (Step 003220): Train loss 2.459, Val loss 5.051\n",
            "Ep 5 (Step 003230): Train loss 2.472, Val loss 5.066\n",
            "Ep 5 (Step 003240): Train loss 2.498, Val loss 5.056\n",
            "Ep 5 (Step 003250): Train loss 2.333, Val loss 5.053\n",
            "Ep 5 (Step 003260): Train loss 2.341, Val loss 5.063\n",
            "Ep 5 (Step 003270): Train loss 2.399, Val loss 5.042\n",
            "Ep 5 (Step 003280): Train loss 2.405, Val loss 5.053\n",
            "Ep 5 (Step 003290): Train loss 2.324, Val loss 5.070\n",
            "Ep 5 (Step 003300): Train loss 2.384, Val loss 5.053\n",
            "\n",
            "Sample generation after epoch 5:\n",
            "<|start|>   <|title|>The Great Tortoise Heist</|title|>  Once upon a time, in a quaint little town named Mr. Johnson, a peculiar event. Mr. Johnson was a tall, imposing man who had a\n",
            "Epoch 5 completed in 71.37 minutes\n",
            "\n",
            "==================================================\n",
            "Epoch 6/12\n",
            "==================================================\n",
            "Ep 6 (Step 003310): Train loss 2.331, Val loss 5.072\n",
            "Ep 6 (Step 003320): Train loss 2.249, Val loss 5.111\n",
            "Ep 6 (Step 003330): Train loss 2.299, Val loss 5.091\n",
            "Ep 6 (Step 003340): Train loss 2.248, Val loss 5.107\n",
            "Ep 6 (Step 003350): Train loss 2.171, Val loss 5.141\n",
            "Ep 6 (Step 003360): Train loss 2.264, Val loss 5.147\n",
            "Ep 6 (Step 003370): Train loss 2.225, Val loss 5.137\n",
            "Ep 6 (Step 003380): Train loss 2.232, Val loss 5.162\n",
            "Ep 6 (Step 003390): Train loss 2.202, Val loss 5.129\n",
            "Ep 6 (Step 003400): Train loss 2.315, Val loss 5.161\n",
            "Ep 6 (Step 003410): Train loss 2.224, Val loss 5.166\n",
            "Ep 6 (Step 003420): Train loss 2.159, Val loss 5.163\n",
            "Ep 6 (Step 003430): Train loss 2.312, Val loss 5.155\n",
            "Ep 6 (Step 003440): Train loss 2.215, Val loss 5.155\n",
            "Ep 6 (Step 003450): Train loss 2.252, Val loss 5.162\n",
            "Ep 6 (Step 003460): Train loss 2.253, Val loss 5.177\n",
            "Ep 6 (Step 003470): Train loss 2.238, Val loss 5.185\n",
            "Ep 6 (Step 003480): Train loss 2.152, Val loss 5.197\n",
            "Ep 6 (Step 003490): Train loss 2.193, Val loss 5.221\n",
            "Ep 6 (Step 003500): Train loss 2.170, Val loss 5.233\n",
            "Ep 6 (Step 003510): Train loss 2.237, Val loss 5.191\n",
            "Ep 6 (Step 003520): Train loss 2.105, Val loss 5.199\n",
            "Ep 6 (Step 003530): Train loss 2.226, Val loss 5.198\n",
            "Ep 6 (Step 003540): Train loss 2.056, Val loss 5.202\n",
            "Ep 6 (Step 003550): Train loss 2.241, Val loss 5.196\n",
            "Ep 6 (Step 003560): Train loss 2.139, Val loss 5.212\n",
            "Ep 6 (Step 003570): Train loss 1.971, Val loss 5.198\n",
            "Ep 6 (Step 003580): Train loss 2.077, Val loss 5.198\n",
            "Ep 6 (Step 003590): Train loss 2.206, Val loss 5.189\n",
            "Ep 6 (Step 003600): Train loss 2.324, Val loss 5.210\n",
            "Ep 6 (Step 003610): Train loss 2.142, Val loss 5.214\n",
            "Ep 6 (Step 003620): Train loss 2.032, Val loss 5.227\n",
            "Ep 6 (Step 003630): Train loss 2.208, Val loss 5.209\n",
            "Ep 6 (Step 003640): Train loss 1.995, Val loss 5.197\n",
            "Ep 6 (Step 003650): Train loss 2.162, Val loss 5.197\n",
            "Ep 6 (Step 003660): Train loss 2.103, Val loss 5.193\n",
            "Ep 6 (Step 003670): Train loss 2.212, Val loss 5.188\n",
            "Ep 6 (Step 003680): Train loss 2.066, Val loss 5.190\n",
            "Ep 6 (Step 003690): Train loss 2.038, Val loss 5.184\n",
            "Ep 6 (Step 003700): Train loss 1.940, Val loss 5.201\n",
            "Ep 6 (Step 003710): Train loss 2.144, Val loss 5.183\n",
            "Ep 6 (Step 003720): Train loss 2.054, Val loss 5.178\n",
            "Ep 6 (Step 003730): Train loss 2.203, Val loss 5.196\n",
            "Ep 6 (Step 003740): Train loss 2.012, Val loss 5.191\n",
            "Ep 6 (Step 003750): Train loss 2.066, Val loss 5.189\n",
            "Ep 6 (Step 003760): Train loss 2.065, Val loss 5.197\n",
            "Ep 6 (Step 003770): Train loss 2.093, Val loss 5.224\n",
            "Ep 6 (Step 003780): Train loss 2.013, Val loss 5.232\n",
            "Ep 6 (Step 003790): Train loss 2.023, Val loss 5.229\n",
            "Ep 6 (Step 003800): Train loss 1.942, Val loss 5.183\n",
            "Ep 6 (Step 003810): Train loss 1.978, Val loss 5.203\n",
            "Ep 6 (Step 003820): Train loss 2.073, Val loss 5.193\n",
            "Ep 6 (Step 003830): Train loss 1.975, Val loss 5.208\n",
            "Ep 6 (Step 003840): Train loss 1.937, Val loss 5.185\n",
            "Ep 6 (Step 003850): Train loss 1.990, Val loss 5.192\n",
            "Ep 6 (Step 003860): Train loss 1.992, Val loss 5.205\n",
            "Ep 6 (Step 003870): Train loss 1.930, Val loss 5.196\n",
            "Ep 6 (Step 003880): Train loss 2.079, Val loss 5.192\n",
            "Ep 6 (Step 003890): Train loss 2.116, Val loss 5.204\n",
            "Ep 6 (Step 003900): Train loss 1.965, Val loss 5.207\n",
            "Ep 6 (Step 003910): Train loss 2.006, Val loss 5.196\n",
            "Ep 6 (Step 003920): Train loss 1.948, Val loss 5.178\n",
            "Ep 6 (Step 003930): Train loss 1.933, Val loss 5.182\n",
            "Ep 6 (Step 003940): Train loss 1.928, Val loss 5.195\n",
            "Ep 6 (Step 003950): Train loss 2.020, Val loss 5.201\n",
            "Ep 6 (Step 003960): Train loss 1.909, Val loss 5.215\n",
            "\n",
            "Sample generation after epoch 6:\n",
            "<|start|>   <|title|>The Unseen</|title|>  In the bustling city of Silicon Valley, a man named Jack lived a man named Jack. Jack was a man of simple life. He was a simple life, working\n",
            "Epoch 6 checkpoint saved\n",
            "Epoch 6 completed in 95.38 minutes\n",
            "\n",
            "==================================================\n",
            "Epoch 7/12\n",
            "==================================================\n",
            "Ep 7 (Step 003970): Train loss 1.852, Val loss 5.226\n",
            "Ep 7 (Step 003980): Train loss 1.875, Val loss 5.241\n",
            "Ep 7 (Step 003990): Train loss 2.027, Val loss 5.264\n",
            "Ep 7 (Step 004000): Train loss 1.840, Val loss 5.286\n",
            "Ep 7 (Step 004010): Train loss 1.940, Val loss 5.293\n",
            "Ep 7 (Step 004020): Train loss 1.828, Val loss 5.301\n",
            "Ep 7 (Step 004030): Train loss 1.733, Val loss 5.286\n",
            "Ep 7 (Step 004040): Train loss 1.831, Val loss 5.292\n",
            "Ep 7 (Step 004050): Train loss 1.743, Val loss 5.307\n",
            "Ep 7 (Step 004060): Train loss 1.749, Val loss 5.312\n",
            "Ep 7 (Step 004070): Train loss 1.731, Val loss 5.334\n",
            "Ep 7 (Step 004080): Train loss 1.661, Val loss 5.340\n",
            "Ep 7 (Step 004090): Train loss 1.773, Val loss 5.356\n",
            "Ep 7 (Step 004100): Train loss 1.828, Val loss 5.346\n",
            "Ep 7 (Step 004110): Train loss 1.859, Val loss 5.375\n",
            "Ep 7 (Step 004120): Train loss 1.771, Val loss 5.356\n",
            "Ep 7 (Step 004130): Train loss 1.766, Val loss 5.364\n",
            "Ep 7 (Step 004140): Train loss 1.871, Val loss 5.385\n",
            "Ep 7 (Step 004150): Train loss 1.808, Val loss 5.368\n",
            "Ep 7 (Step 004160): Train loss 1.819, Val loss 5.367\n",
            "Ep 7 (Step 004170): Train loss 1.880, Val loss 5.373\n",
            "Ep 7 (Step 004180): Train loss 1.830, Val loss 5.356\n",
            "Ep 7 (Step 004190): Train loss 1.808, Val loss 5.361\n",
            "Ep 7 (Step 004200): Train loss 1.835, Val loss 5.375\n",
            "Ep 7 (Step 004210): Train loss 1.728, Val loss 5.362\n",
            "Ep 7 (Step 004220): Train loss 1.740, Val loss 5.364\n",
            "Ep 7 (Step 004230): Train loss 1.687, Val loss 5.384\n",
            "Ep 7 (Step 004240): Train loss 1.772, Val loss 5.386\n",
            "Ep 7 (Step 004250): Train loss 1.574, Val loss 5.388\n",
            "Ep 7 (Step 004260): Train loss 1.807, Val loss 5.396\n",
            "Ep 7 (Step 004270): Train loss 1.691, Val loss 5.404\n",
            "Ep 7 (Step 004280): Train loss 1.706, Val loss 5.411\n",
            "Ep 7 (Step 004290): Train loss 1.651, Val loss 5.426\n",
            "Ep 7 (Step 004300): Train loss 1.651, Val loss 5.403\n",
            "Ep 7 (Step 004310): Train loss 1.621, Val loss 5.433\n",
            "Ep 7 (Step 004320): Train loss 1.677, Val loss 5.408\n",
            "Ep 7 (Step 004330): Train loss 1.747, Val loss 5.426\n",
            "Ep 7 (Step 004340): Train loss 1.777, Val loss 5.423\n",
            "Ep 7 (Step 004350): Train loss 1.692, Val loss 5.404\n",
            "Ep 7 (Step 004360): Train loss 1.776, Val loss 5.401\n",
            "Ep 7 (Step 004370): Train loss 1.656, Val loss 5.408\n",
            "Ep 7 (Step 004380): Train loss 1.787, Val loss 5.390\n",
            "Ep 7 (Step 004390): Train loss 1.622, Val loss 5.414\n",
            "Ep 7 (Step 004400): Train loss 1.723, Val loss 5.409\n",
            "Ep 7 (Step 004410): Train loss 1.748, Val loss 5.399\n",
            "Ep 7 (Step 004420): Train loss 1.706, Val loss 5.400\n",
            "Ep 7 (Step 004430): Train loss 1.720, Val loss 5.394\n",
            "Ep 7 (Step 004440): Train loss 1.622, Val loss 5.393\n",
            "Ep 7 (Step 004450): Train loss 1.661, Val loss 5.393\n",
            "Ep 7 (Step 004460): Train loss 1.683, Val loss 5.398\n",
            "Ep 7 (Step 004470): Train loss 1.581, Val loss 5.396\n",
            "Ep 7 (Step 004480): Train loss 1.722, Val loss 5.404\n",
            "Ep 7 (Step 004490): Train loss 1.690, Val loss 5.394\n",
            "Ep 7 (Step 004500): Train loss 1.653, Val loss 5.402\n",
            "Ep 7 (Step 004510): Train loss 1.592, Val loss 5.404\n",
            "Ep 7 (Step 004520): Train loss 1.774, Val loss 5.412\n",
            "Ep 7 (Step 004530): Train loss 1.549, Val loss 5.383\n",
            "Ep 7 (Step 004540): Train loss 1.582, Val loss 5.411\n",
            "Ep 7 (Step 004550): Train loss 1.712, Val loss 5.415\n",
            "Ep 7 (Step 004560): Train loss 1.615, Val loss 5.404\n",
            "Ep 7 (Step 004570): Train loss 1.680, Val loss 5.406\n",
            "Ep 7 (Step 004580): Train loss 1.640, Val loss 5.396\n",
            "Ep 7 (Step 004590): Train loss 1.507, Val loss 5.395\n",
            "Ep 7 (Step 004600): Train loss 1.516, Val loss 5.387\n",
            "Ep 7 (Step 004610): Train loss 1.584, Val loss 5.381\n",
            "Ep 7 (Step 004620): Train loss 1.459, Val loss 5.364\n",
            "\n",
            "Sample generation after epoch 7:\n",
            "<|start|>   <|title|>The Chronicles of the Forgotten Realm</|title|>  Once upon a time, in the small, quaint town of Serenity, there lived a young girl named Lila. Lila was a kind\n",
            "Epoch 7 completed in 119.22 minutes\n",
            "\n",
            "==================================================\n",
            "Epoch 8/12\n",
            "==================================================\n",
            "Ep 8 (Step 004630): Train loss 1.500, Val loss 5.395\n",
            "Ep 8 (Step 004640): Train loss 1.421, Val loss 5.415\n",
            "Ep 8 (Step 004650): Train loss 1.492, Val loss 5.462\n",
            "Ep 8 (Step 004660): Train loss 1.490, Val loss 5.478\n",
            "Ep 8 (Step 004670): Train loss 1.510, Val loss 5.490\n",
            "Ep 8 (Step 004680): Train loss 1.459, Val loss 5.503\n",
            "Ep 8 (Step 004690): Train loss 1.502, Val loss 5.519\n",
            "Ep 8 (Step 004700): Train loss 1.465, Val loss 5.534\n",
            "Ep 8 (Step 004710): Train loss 1.365, Val loss 5.532\n",
            "Ep 8 (Step 004720): Train loss 1.376, Val loss 5.539\n",
            "Ep 8 (Step 004730): Train loss 1.478, Val loss 5.547\n",
            "Ep 8 (Step 004740): Train loss 1.378, Val loss 5.545\n",
            "Ep 8 (Step 004750): Train loss 1.390, Val loss 5.562\n",
            "Ep 8 (Step 004760): Train loss 1.398, Val loss 5.573\n",
            "Ep 8 (Step 004770): Train loss 1.382, Val loss 5.576\n",
            "Ep 8 (Step 004780): Train loss 1.410, Val loss 5.581\n",
            "Ep 8 (Step 004790): Train loss 1.401, Val loss 5.598\n",
            "Ep 8 (Step 004800): Train loss 1.418, Val loss 5.582\n",
            "Ep 8 (Step 004810): Train loss 1.522, Val loss 5.592\n",
            "Ep 8 (Step 004820): Train loss 1.495, Val loss 5.601\n",
            "Ep 8 (Step 004830): Train loss 1.393, Val loss 5.611\n",
            "Ep 8 (Step 004840): Train loss 1.464, Val loss 5.648\n",
            "Ep 8 (Step 004850): Train loss 1.363, Val loss 5.616\n",
            "Ep 8 (Step 004860): Train loss 1.408, Val loss 5.618\n",
            "Ep 8 (Step 004870): Train loss 1.336, Val loss 5.626\n",
            "Ep 8 (Step 004880): Train loss 1.382, Val loss 5.630\n",
            "Ep 8 (Step 004890): Train loss 1.345, Val loss 5.611\n",
            "Ep 8 (Step 004900): Train loss 1.368, Val loss 5.611\n",
            "Ep 8 (Step 004910): Train loss 1.518, Val loss 5.622\n",
            "Ep 8 (Step 004920): Train loss 1.364, Val loss 5.633\n",
            "Ep 8 (Step 004930): Train loss 1.400, Val loss 5.607\n",
            "Ep 8 (Step 004940): Train loss 1.409, Val loss 5.631\n",
            "Ep 8 (Step 004950): Train loss 1.345, Val loss 5.614\n",
            "Ep 8 (Step 004960): Train loss 1.334, Val loss 5.626\n",
            "Ep 8 (Step 004970): Train loss 1.275, Val loss 5.623\n",
            "Ep 8 (Step 004980): Train loss 1.351, Val loss 5.630\n",
            "Ep 8 (Step 004990): Train loss 1.316, Val loss 5.634\n",
            "Ep 8 (Step 005000): Train loss 1.359, Val loss 5.627\n",
            "Ep 8 (Step 005010): Train loss 1.343, Val loss 5.615\n",
            "Ep 8 (Step 005020): Train loss 1.278, Val loss 5.620\n",
            "Ep 8 (Step 005030): Train loss 1.379, Val loss 5.633\n",
            "Ep 8 (Step 005040): Train loss 1.266, Val loss 5.637\n",
            "Ep 8 (Step 005050): Train loss 1.377, Val loss 5.634\n",
            "Ep 8 (Step 005060): Train loss 1.431, Val loss 5.632\n",
            "Ep 8 (Step 005070): Train loss 1.333, Val loss 5.627\n",
            "Ep 8 (Step 005080): Train loss 1.356, Val loss 5.644\n",
            "Ep 8 (Step 005090): Train loss 1.336, Val loss 5.649\n",
            "Ep 8 (Step 005100): Train loss 1.285, Val loss 5.639\n",
            "Ep 8 (Step 005110): Train loss 1.310, Val loss 5.623\n",
            "Ep 8 (Step 005120): Train loss 1.421, Val loss 5.623\n",
            "Ep 8 (Step 005130): Train loss 1.304, Val loss 5.651\n",
            "Ep 8 (Step 005140): Train loss 1.257, Val loss 5.648\n",
            "Ep 8 (Step 005150): Train loss 1.286, Val loss 5.656\n",
            "Ep 8 (Step 005160): Train loss 1.158, Val loss 5.651\n",
            "Ep 8 (Step 005170): Train loss 1.203, Val loss 5.646\n",
            "Ep 8 (Step 005180): Train loss 1.289, Val loss 5.661\n",
            "Ep 8 (Step 005190): Train loss 1.303, Val loss 5.644\n",
            "Ep 8 (Step 005200): Train loss 1.216, Val loss 5.641\n",
            "Ep 8 (Step 005210): Train loss 1.288, Val loss 5.630\n",
            "Ep 8 (Step 005220): Train loss 1.232, Val loss 5.636\n",
            "Ep 8 (Step 005230): Train loss 1.266, Val loss 5.652\n",
            "Ep 8 (Step 005240): Train loss 1.187, Val loss 5.621\n",
            "Ep 8 (Step 005250): Train loss 1.259, Val loss 5.645\n",
            "Ep 8 (Step 005260): Train loss 1.200, Val loss 5.642\n",
            "Ep 8 (Step 005270): Train loss 1.243, Val loss 5.659\n",
            "Ep 8 (Step 005280): Train loss 1.184, Val loss 5.637\n",
            "\n",
            "Sample generation after epoch 8:\n",
            "<|start|>   <|title|>The Great Tortoise Heist</|title|>  In the small town of Nutville, there lived an unlikely hero named Alaric. He was a librarian at the local library, and his wife\n",
            "Epoch 8 checkpoint saved\n",
            "Epoch 8 completed in 143.29 minutes\n",
            "\n",
            "==================================================\n",
            "Epoch 9/12\n",
            "==================================================\n",
            "Ep 9 (Step 005290): Train loss 1.271, Val loss 5.687\n",
            "Ep 9 (Step 005300): Train loss 1.297, Val loss 5.691\n",
            "Ep 9 (Step 005310): Train loss 1.201, Val loss 5.727\n",
            "Ep 9 (Step 005320): Train loss 1.125, Val loss 5.732\n",
            "Ep 9 (Step 005330): Train loss 1.291, Val loss 5.766\n",
            "Ep 9 (Step 005340): Train loss 1.084, Val loss 5.781\n",
            "Ep 9 (Step 005350): Train loss 1.047, Val loss 5.776\n",
            "Ep 9 (Step 005360): Train loss 1.071, Val loss 5.780\n",
            "Ep 9 (Step 005370): Train loss 1.015, Val loss 5.777\n",
            "Ep 9 (Step 005380): Train loss 1.143, Val loss 5.794\n",
            "Ep 9 (Step 005390): Train loss 1.195, Val loss 5.794\n",
            "Ep 9 (Step 005400): Train loss 1.000, Val loss 5.822\n",
            "Ep 9 (Step 005410): Train loss 1.049, Val loss 5.823\n",
            "Ep 9 (Step 005420): Train loss 1.186, Val loss 5.827\n",
            "Ep 9 (Step 005430): Train loss 1.272, Val loss 5.833\n",
            "Ep 9 (Step 005440): Train loss 1.082, Val loss 5.841\n",
            "Ep 9 (Step 005450): Train loss 0.999, Val loss 5.852\n",
            "Ep 9 (Step 005460): Train loss 1.066, Val loss 5.845\n",
            "Ep 9 (Step 005470): Train loss 1.028, Val loss 5.839\n",
            "Ep 9 (Step 005480): Train loss 1.102, Val loss 5.864\n",
            "Ep 9 (Step 005490): Train loss 1.144, Val loss 5.871\n",
            "Ep 9 (Step 005500): Train loss 1.025, Val loss 5.873\n",
            "Ep 9 (Step 005510): Train loss 1.040, Val loss 5.886\n",
            "Ep 9 (Step 005520): Train loss 1.051, Val loss 5.877\n",
            "Ep 9 (Step 005530): Train loss 1.010, Val loss 5.916\n",
            "Ep 9 (Step 005540): Train loss 1.076, Val loss 5.902\n",
            "Ep 9 (Step 005550): Train loss 1.047, Val loss 5.886\n",
            "Ep 9 (Step 005560): Train loss 1.118, Val loss 5.885\n",
            "Ep 9 (Step 005570): Train loss 0.957, Val loss 5.885\n",
            "Ep 9 (Step 005580): Train loss 1.136, Val loss 5.892\n",
            "Ep 9 (Step 005590): Train loss 1.039, Val loss 5.927\n",
            "Ep 9 (Step 005600): Train loss 1.015, Val loss 5.917\n",
            "Ep 9 (Step 005610): Train loss 1.033, Val loss 5.888\n",
            "Ep 9 (Step 005620): Train loss 1.107, Val loss 5.900\n",
            "Ep 9 (Step 005630): Train loss 1.046, Val loss 5.888\n",
            "Ep 9 (Step 005640): Train loss 1.062, Val loss 5.914\n",
            "Ep 9 (Step 005650): Train loss 1.068, Val loss 5.915\n",
            "Ep 9 (Step 005660): Train loss 1.051, Val loss 5.906\n",
            "Ep 9 (Step 005670): Train loss 1.068, Val loss 5.904\n",
            "Ep 9 (Step 005680): Train loss 1.095, Val loss 5.904\n",
            "Ep 9 (Step 005690): Train loss 1.035, Val loss 5.899\n",
            "Ep 9 (Step 005700): Train loss 1.044, Val loss 5.932\n",
            "Ep 9 (Step 005710): Train loss 1.006, Val loss 5.926\n",
            "Ep 9 (Step 005720): Train loss 1.041, Val loss 5.896\n",
            "Ep 9 (Step 005730): Train loss 1.071, Val loss 5.903\n",
            "Ep 9 (Step 005740): Train loss 1.024, Val loss 5.908\n",
            "Ep 9 (Step 005750): Train loss 0.988, Val loss 5.904\n",
            "Ep 9 (Step 005760): Train loss 1.000, Val loss 5.924\n",
            "Ep 9 (Step 005770): Train loss 1.020, Val loss 5.895\n",
            "Ep 9 (Step 005780): Train loss 1.051, Val loss 5.939\n",
            "Ep 9 (Step 005790): Train loss 0.931, Val loss 5.932\n",
            "Ep 9 (Step 005800): Train loss 1.039, Val loss 5.897\n",
            "Ep 9 (Step 005810): Train loss 1.021, Val loss 5.915\n",
            "Ep 9 (Step 005820): Train loss 0.993, Val loss 5.913\n",
            "Ep 9 (Step 005830): Train loss 0.942, Val loss 5.921\n",
            "Ep 9 (Step 005840): Train loss 1.046, Val loss 5.912\n",
            "Ep 9 (Step 005850): Train loss 0.952, Val loss 5.908\n",
            "Ep 9 (Step 005860): Train loss 0.954, Val loss 5.910\n",
            "Ep 9 (Step 005870): Train loss 0.945, Val loss 5.899\n",
            "Ep 9 (Step 005880): Train loss 0.950, Val loss 5.897\n",
            "Ep 9 (Step 005890): Train loss 1.021, Val loss 5.880\n",
            "Ep 9 (Step 005900): Train loss 1.060, Val loss 5.913\n",
            "Ep 9 (Step 005910): Train loss 0.931, Val loss 5.891\n",
            "Ep 9 (Step 005920): Train loss 1.036, Val loss 5.908\n",
            "Ep 9 (Step 005930): Train loss 0.901, Val loss 5.908\n",
            "Ep 9 (Step 005940): Train loss 0.890, Val loss 5.916\n",
            "\n",
            "Sample generation after epoch 9:\n",
            "<|start|>   <|title|>The Great Past of the Celestial Eagle</|title|>  In the year of the realm, and the world was once again a place where the sun shone above the horizon, and the skies were filled with\n",
            "Epoch 9 completed in 167.15 minutes\n",
            "\n",
            "==================================================\n",
            "Epoch 10/12\n",
            "==================================================\n",
            "Ep 10 (Step 005950): Train loss 0.977, Val loss 5.910\n",
            "Ep 10 (Step 005960): Train loss 0.970, Val loss 5.954\n",
            "Ep 10 (Step 005970): Train loss 0.970, Val loss 5.973\n",
            "Ep 10 (Step 005980): Train loss 0.835, Val loss 6.005\n",
            "Ep 10 (Step 005990): Train loss 0.832, Val loss 6.019\n",
            "Ep 10 (Step 006000): Train loss 0.860, Val loss 6.032\n",
            "Ep 10 (Step 006010): Train loss 0.759, Val loss 6.054\n",
            "Ep 10 (Step 006020): Train loss 0.865, Val loss 6.069\n",
            "Ep 10 (Step 006030): Train loss 0.849, Val loss 6.091\n",
            "Ep 10 (Step 006040): Train loss 0.880, Val loss 6.090\n",
            "Ep 10 (Step 006050): Train loss 0.807, Val loss 6.098\n",
            "Ep 10 (Step 006060): Train loss 0.860, Val loss 6.095\n",
            "Ep 10 (Step 006070): Train loss 0.811, Val loss 6.109\n",
            "Ep 10 (Step 006080): Train loss 0.881, Val loss 6.109\n",
            "Ep 10 (Step 006090): Train loss 0.807, Val loss 6.134\n",
            "Ep 10 (Step 006100): Train loss 0.873, Val loss 6.157\n",
            "Ep 10 (Step 006110): Train loss 0.830, Val loss 6.146\n",
            "Ep 10 (Step 006120): Train loss 0.796, Val loss 6.145\n",
            "Ep 10 (Step 006130): Train loss 0.805, Val loss 6.160\n",
            "Ep 10 (Step 006140): Train loss 0.827, Val loss 6.131\n",
            "Ep 10 (Step 006150): Train loss 0.782, Val loss 6.135\n",
            "Ep 10 (Step 006160): Train loss 0.829, Val loss 6.156\n",
            "Ep 10 (Step 006170): Train loss 0.858, Val loss 6.171\n",
            "Ep 10 (Step 006180): Train loss 0.818, Val loss 6.157\n",
            "Ep 10 (Step 006190): Train loss 0.815, Val loss 6.171\n",
            "Ep 10 (Step 006200): Train loss 0.865, Val loss 6.172\n",
            "Ep 10 (Step 006210): Train loss 0.804, Val loss 6.196\n",
            "Ep 10 (Step 006220): Train loss 0.779, Val loss 6.182\n",
            "Ep 10 (Step 006230): Train loss 0.863, Val loss 6.200\n",
            "Ep 10 (Step 006240): Train loss 0.872, Val loss 6.174\n",
            "Ep 10 (Step 006250): Train loss 0.771, Val loss 6.184\n",
            "Ep 10 (Step 006260): Train loss 0.797, Val loss 6.180\n",
            "Ep 10 (Step 006270): Train loss 0.762, Val loss 6.194\n",
            "Ep 10 (Step 006280): Train loss 0.908, Val loss 6.203\n",
            "Ep 10 (Step 006290): Train loss 0.780, Val loss 6.203\n",
            "Ep 10 (Step 006300): Train loss 0.823, Val loss 6.204\n",
            "Ep 10 (Step 006310): Train loss 0.799, Val loss 6.203\n",
            "Ep 10 (Step 006320): Train loss 0.789, Val loss 6.205\n",
            "Ep 10 (Step 006330): Train loss 0.776, Val loss 6.181\n",
            "Ep 10 (Step 006340): Train loss 0.753, Val loss 6.199\n",
            "Ep 10 (Step 006350): Train loss 0.788, Val loss 6.205\n",
            "Ep 10 (Step 006360): Train loss 0.687, Val loss 6.201\n",
            "Ep 10 (Step 006370): Train loss 0.838, Val loss 6.207\n",
            "Ep 10 (Step 006380): Train loss 0.721, Val loss 6.185\n",
            "Ep 10 (Step 006390): Train loss 0.810, Val loss 6.189\n",
            "Ep 10 (Step 006400): Train loss 0.836, Val loss 6.211\n",
            "Ep 10 (Step 006410): Train loss 0.789, Val loss 6.207\n",
            "Ep 10 (Step 006420): Train loss 0.774, Val loss 6.197\n",
            "Ep 10 (Step 006430): Train loss 0.816, Val loss 6.212\n",
            "Ep 10 (Step 006440): Train loss 0.747, Val loss 6.206\n",
            "Ep 10 (Step 006450): Train loss 0.772, Val loss 6.201\n",
            "Ep 10 (Step 006460): Train loss 0.682, Val loss 6.201\n",
            "Ep 10 (Step 006470): Train loss 0.761, Val loss 6.190\n",
            "Ep 10 (Step 006480): Train loss 0.704, Val loss 6.172\n",
            "Ep 10 (Step 006490): Train loss 0.807, Val loss 6.178\n",
            "Ep 10 (Step 006500): Train loss 0.782, Val loss 6.197\n",
            "Ep 10 (Step 006510): Train loss 0.801, Val loss 6.177\n",
            "Ep 10 (Step 006520): Train loss 0.771, Val loss 6.201\n",
            "Ep 10 (Step 006530): Train loss 0.718, Val loss 6.193\n",
            "Ep 10 (Step 006540): Train loss 0.786, Val loss 6.185\n",
            "Ep 10 (Step 006550): Train loss 0.763, Val loss 6.205\n",
            "Ep 10 (Step 006560): Train loss 0.718, Val loss 6.197\n",
            "Ep 10 (Step 006570): Train loss 0.722, Val loss 6.166\n",
            "Ep 10 (Step 006580): Train loss 0.735, Val loss 6.173\n",
            "Ep 10 (Step 006590): Train loss 0.710, Val loss 6.190\n",
            "Ep 10 (Step 006600): Train loss 0.706, Val loss 6.165\n",
            "\n",
            "Sample generation after epoch 10:\n",
            "<|start|>   <|title|>The Unyielding Flame of Courage</|title|>  In the bustling city of New York, a young inventor named Archibald was working tirelessly on a groundbreaking project. His work was work, and\n",
            "Epoch 10 checkpoint saved\n",
            "Epoch 10 completed in 191.16 minutes\n",
            "\n",
            "==================================================\n",
            "Epoch 11/12\n",
            "==================================================\n",
            "Ep 11 (Step 006610): Train loss 0.704, Val loss 6.186\n",
            "Ep 11 (Step 006620): Train loss 0.676, Val loss 6.234\n",
            "Ep 11 (Step 006630): Train loss 0.676, Val loss 6.278\n",
            "Ep 11 (Step 006640): Train loss 0.696, Val loss 6.285\n",
            "Ep 11 (Step 006650): Train loss 0.605, Val loss 6.276\n",
            "Ep 11 (Step 006660): Train loss 0.707, Val loss 6.290\n",
            "Ep 11 (Step 006670): Train loss 0.615, Val loss 6.301\n",
            "Ep 11 (Step 006680): Train loss 0.624, Val loss 6.313\n",
            "Ep 11 (Step 006690): Train loss 0.643, Val loss 6.322\n",
            "Ep 11 (Step 006700): Train loss 0.596, Val loss 6.321\n",
            "Ep 11 (Step 006710): Train loss 0.652, Val loss 6.345\n",
            "Ep 11 (Step 006720): Train loss 0.634, Val loss 6.359\n",
            "Ep 11 (Step 006730): Train loss 0.582, Val loss 6.364\n",
            "Ep 11 (Step 006740): Train loss 0.677, Val loss 6.369\n",
            "Ep 11 (Step 006750): Train loss 0.650, Val loss 6.368\n",
            "Ep 11 (Step 006760): Train loss 0.662, Val loss 6.390\n",
            "Ep 11 (Step 006770): Train loss 0.666, Val loss 6.408\n",
            "Ep 11 (Step 006780): Train loss 0.625, Val loss 6.422\n",
            "Ep 11 (Step 006790): Train loss 0.671, Val loss 6.413\n",
            "Ep 11 (Step 006800): Train loss 0.657, Val loss 6.410\n",
            "Ep 11 (Step 006810): Train loss 0.589, Val loss 6.418\n",
            "Ep 11 (Step 006820): Train loss 0.560, Val loss 6.414\n",
            "Ep 11 (Step 006830): Train loss 0.576, Val loss 6.425\n",
            "Ep 11 (Step 006840): Train loss 0.597, Val loss 6.438\n",
            "Ep 11 (Step 006850): Train loss 0.612, Val loss 6.447\n",
            "Ep 11 (Step 006860): Train loss 0.668, Val loss 6.459\n",
            "Ep 11 (Step 006870): Train loss 0.537, Val loss 6.433\n",
            "Ep 11 (Step 006880): Train loss 0.626, Val loss 6.448\n",
            "Ep 11 (Step 006890): Train loss 0.558, Val loss 6.461\n",
            "Ep 11 (Step 006900): Train loss 0.578, Val loss 6.449\n",
            "Ep 11 (Step 006910): Train loss 0.580, Val loss 6.457\n",
            "Ep 11 (Step 006920): Train loss 0.595, Val loss 6.466\n",
            "Ep 11 (Step 006930): Train loss 0.576, Val loss 6.458\n",
            "Ep 11 (Step 006940): Train loss 0.606, Val loss 6.468\n",
            "Ep 11 (Step 006950): Train loss 0.600, Val loss 6.485\n",
            "Ep 11 (Step 006960): Train loss 0.579, Val loss 6.458\n",
            "Ep 11 (Step 006970): Train loss 0.566, Val loss 6.465\n",
            "Ep 11 (Step 006980): Train loss 0.551, Val loss 6.450\n",
            "Ep 11 (Step 006990): Train loss 0.593, Val loss 6.458\n",
            "Ep 11 (Step 007000): Train loss 0.632, Val loss 6.462\n",
            "Ep 11 (Step 007010): Train loss 0.640, Val loss 6.480\n",
            "Ep 11 (Step 007020): Train loss 0.577, Val loss 6.491\n",
            "Ep 11 (Step 007030): Train loss 0.594, Val loss 6.471\n",
            "Ep 11 (Step 007040): Train loss 0.571, Val loss 6.458\n",
            "Ep 11 (Step 007050): Train loss 0.632, Val loss 6.472\n",
            "Ep 11 (Step 007060): Train loss 0.588, Val loss 6.477\n",
            "Ep 11 (Step 007070): Train loss 0.495, Val loss 6.470\n",
            "Ep 11 (Step 007080): Train loss 0.556, Val loss 6.457\n",
            "Ep 11 (Step 007090): Train loss 0.609, Val loss 6.465\n",
            "Ep 11 (Step 007100): Train loss 0.517, Val loss 6.472\n",
            "Ep 11 (Step 007110): Train loss 0.540, Val loss 6.459\n",
            "Ep 11 (Step 007120): Train loss 0.624, Val loss 6.486\n",
            "Ep 11 (Step 007130): Train loss 0.627, Val loss 6.467\n",
            "Ep 11 (Step 007140): Train loss 0.569, Val loss 6.473\n",
            "Ep 11 (Step 007150): Train loss 0.612, Val loss 6.473\n",
            "Ep 11 (Step 007160): Train loss 0.523, Val loss 6.483\n",
            "Ep 11 (Step 007170): Train loss 0.502, Val loss 6.471\n",
            "Ep 11 (Step 007180): Train loss 0.587, Val loss 6.488\n",
            "Ep 11 (Step 007190): Train loss 0.555, Val loss 6.477\n",
            "Ep 11 (Step 007200): Train loss 0.570, Val loss 6.495\n",
            "Ep 11 (Step 007210): Train loss 0.554, Val loss 6.476\n",
            "Ep 11 (Step 007220): Train loss 0.589, Val loss 6.466\n",
            "Ep 11 (Step 007230): Train loss 0.571, Val loss 6.471\n",
            "Ep 11 (Step 007240): Train loss 0.585, Val loss 6.481\n",
            "Ep 11 (Step 007250): Train loss 0.570, Val loss 6.474\n",
            "Ep 11 (Step 007260): Train loss 0.531, Val loss 6.453\n",
            "Ep 11 (Step 007270): Train loss 0.552, Val loss 6.472\n",
            "\n",
            "Sample generation after epoch 11:\n",
            "<|start|>   <|title|>The Unraveling Truth</|title|>  In the bustling city of New York, a prominent advertising agency known as New Haven. The Newhaven was a high-stakes operation, with the city's\n",
            "Epoch 11 completed in 215.19 minutes\n",
            "\n",
            "==================================================\n",
            "Epoch 12/12\n",
            "==================================================\n",
            "Ep 12 (Step 007280): Train loss 0.536, Val loss 6.538\n",
            "Ep 12 (Step 007290): Train loss 0.480, Val loss 6.541\n",
            "Ep 12 (Step 007300): Train loss 0.447, Val loss 6.574\n",
            "Ep 12 (Step 007310): Train loss 0.489, Val loss 6.566\n",
            "Ep 12 (Step 007320): Train loss 0.464, Val loss 6.577\n",
            "Ep 12 (Step 007330): Train loss 0.485, Val loss 6.603\n",
            "Ep 12 (Step 007340): Train loss 0.436, Val loss 6.623\n",
            "Ep 12 (Step 007350): Train loss 0.491, Val loss 6.637\n",
            "Ep 12 (Step 007360): Train loss 0.425, Val loss 6.624\n",
            "Ep 12 (Step 007370): Train loss 0.438, Val loss 6.635\n",
            "Ep 12 (Step 007380): Train loss 0.478, Val loss 6.635\n",
            "Ep 12 (Step 007390): Train loss 0.440, Val loss 6.651\n",
            "Ep 12 (Step 007400): Train loss 0.421, Val loss 6.653\n",
            "Ep 12 (Step 007410): Train loss 0.464, Val loss 6.677\n",
            "Ep 12 (Step 007420): Train loss 0.430, Val loss 6.698\n",
            "Ep 12 (Step 007430): Train loss 0.465, Val loss 6.704\n",
            "Ep 12 (Step 007440): Train loss 0.459, Val loss 6.707\n",
            "Ep 12 (Step 007450): Train loss 0.442, Val loss 6.706\n",
            "Ep 12 (Step 007460): Train loss 0.407, Val loss 6.725\n",
            "Ep 12 (Step 007470): Train loss 0.465, Val loss 6.706\n",
            "Ep 12 (Step 007480): Train loss 0.508, Val loss 6.713\n",
            "Ep 12 (Step 007490): Train loss 0.497, Val loss 6.724\n",
            "Ep 12 (Step 007500): Train loss 0.439, Val loss 6.718\n",
            "Ep 12 (Step 007510): Train loss 0.485, Val loss 6.731\n",
            "Ep 12 (Step 007520): Train loss 0.457, Val loss 6.725\n",
            "Ep 12 (Step 007530): Train loss 0.435, Val loss 6.746\n",
            "Ep 12 (Step 007540): Train loss 0.469, Val loss 6.739\n",
            "Ep 12 (Step 007550): Train loss 0.447, Val loss 6.763\n",
            "Ep 12 (Step 007560): Train loss 0.461, Val loss 6.715\n",
            "Ep 12 (Step 007570): Train loss 0.440, Val loss 6.723\n",
            "Ep 12 (Step 007580): Train loss 0.504, Val loss 6.743\n",
            "Ep 12 (Step 007590): Train loss 0.461, Val loss 6.754\n",
            "Ep 12 (Step 007600): Train loss 0.400, Val loss 6.763\n",
            "Ep 12 (Step 007610): Train loss 0.493, Val loss 6.747\n",
            "Ep 12 (Step 007620): Train loss 0.523, Val loss 6.729\n",
            "Ep 12 (Step 007630): Train loss 0.457, Val loss 6.737\n",
            "Ep 12 (Step 007640): Train loss 0.462, Val loss 6.759\n",
            "Ep 12 (Step 007650): Train loss 0.449, Val loss 6.754\n",
            "Ep 12 (Step 007660): Train loss 0.491, Val loss 6.766\n",
            "Ep 12 (Step 007670): Train loss 0.454, Val loss 6.759\n",
            "Ep 12 (Step 007680): Train loss 0.477, Val loss 6.777\n",
            "Ep 12 (Step 007690): Train loss 0.442, Val loss 6.744\n",
            "Ep 12 (Step 007700): Train loss 0.405, Val loss 6.761\n",
            "Ep 12 (Step 007710): Train loss 0.480, Val loss 6.762\n",
            "Ep 12 (Step 007720): Train loss 0.419, Val loss 6.757\n",
            "Ep 12 (Step 007730): Train loss 0.437, Val loss 6.748\n",
            "Ep 12 (Step 007740): Train loss 0.462, Val loss 6.748\n",
            "Ep 12 (Step 007750): Train loss 0.450, Val loss 6.726\n",
            "Ep 12 (Step 007760): Train loss 0.449, Val loss 6.734\n",
            "Ep 12 (Step 007770): Train loss 0.393, Val loss 6.752\n",
            "Ep 12 (Step 007780): Train loss 0.432, Val loss 6.751\n",
            "Ep 12 (Step 007790): Train loss 0.401, Val loss 6.737\n",
            "Ep 12 (Step 007800): Train loss 0.427, Val loss 6.744\n",
            "Ep 12 (Step 007810): Train loss 0.393, Val loss 6.759\n",
            "Ep 12 (Step 007820): Train loss 0.403, Val loss 6.721\n",
            "Ep 12 (Step 007830): Train loss 0.400, Val loss 6.720\n",
            "Ep 12 (Step 007840): Train loss 0.395, Val loss 6.749\n",
            "Ep 12 (Step 007850): Train loss 0.420, Val loss 6.704\n",
            "Ep 12 (Step 007860): Train loss 0.430, Val loss 6.720\n",
            "Ep 12 (Step 007870): Train loss 0.392, Val loss 6.729\n",
            "Ep 12 (Step 007880): Train loss 0.405, Val loss 6.717\n",
            "Ep 12 (Step 007890): Train loss 0.377, Val loss 6.715\n",
            "Ep 12 (Step 007900): Train loss 0.421, Val loss 6.732\n",
            "Ep 12 (Step 007910): Train loss 0.416, Val loss 6.725\n",
            "Ep 12 (Step 007920): Train loss 0.419, Val loss 6.726\n",
            "Ep 12 (Step 007930): Train loss 0.418, Val loss 6.724\n",
            "\n",
            "Sample generation after epoch 12:\n",
            "<|start|>   <|title|>The Unyielding Flame of Courage</|title|>  In the small, idyllic town of Willowbrook, a peculiar country had been largely due to a prestigious university, the Willowbrook had been\n",
            "Epoch 12 checkpoint saved\n",
            "Epoch 12 completed in 239.27 minutes\n",
            "Training completed in 239.82 minutes.\n"
          ]
        }
      ],
      "source": [
        "# Note:\n",
        "# Uncomment the following code to calculate the execution time\n",
        "\n",
        "train_start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 12\n",
        "checkpoint_dir = project_path+'/gpt_checkpoints'\n",
        "\n",
        "\n",
        "checkpoint_file = 'checkpoint_V1_2_epoch_2_step_1321.pt'\n",
        "\n",
        "train_losses, val_losses, tokens_seen = start_fresh_training()\n",
        "# train_losses, val_losses, tokens_seen = resume_training_from_checkpoint(checkpoint_dir,checkpoint_file)\n",
        "\n",
        "\n",
        "# Print Time\n",
        "execution_time_minutes = (time.time() - train_start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "809417b6",
      "metadata": {},
      "source": [
        "**Total time was 49.68 + 239.27 = 288.95 minutes = ~ 4.816 hours**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd889564",
      "metadata": {},
      "source": [
        "## **Plot the Loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oxYWyCOEw9OO",
      "metadata": {
        "id": "oxYWyCOEw9OO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, train_losses, val_losses):\n",
        "    fig, ax = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "\n",
        "    ax.set_xlabel(\"Epochs\")\n",
        "    ax.set_ylabel(\"Loss\")\n",
        "    ax.legend(loc=\"upper right\")\n",
        "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.savefig(\"loss-plot.png\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8f36e22",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, train_losses, val_losses)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
